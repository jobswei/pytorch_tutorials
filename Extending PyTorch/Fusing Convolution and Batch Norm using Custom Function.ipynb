{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们将展示一种可以在训练期间应用的融合两层的不同技术。此优化的目标不是缩短运行时间，而是减少内存使用量。\n",
    "\n",
    "# 我们通过将卷积和批量规范组合成单个层（作为自定义函数）来避免这种额外分配。在这个组合层的前向中，我们按原样执行正常卷积和批量规范，唯一的区别是我们只会保存卷积的输入。为了获得批量规范的输入（这是反向传播所必需的），我们在反向传播期间再次重新计算前向卷积。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积的反向公式实现\n",
    "import torch\n",
    "from torch.autograd.function import once_differentiable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def convolution_backward(grad_out, X, weight):\n",
    "    grad_input = F.conv2d(X.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n",
    "    grad_X = F.conv_transpose2d(grad_out, weight)\n",
    "    return grad_X, grad_input\n",
    "\n",
    "class Conv2D(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, weight):\n",
    "        ctx.save_for_backward(X, weight)\n",
    "        return F.conv2d(X, weight)\n",
    "\n",
    "    # Use @once_differentiable by default unless we intend to double backward\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, weight = ctx.saved_tensors\n",
    "        return convolution_backward(grad_out, X, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试时gradcheck，使用双精度很重要\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(10, 3, 7, 7, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(Conv2D.apply, (X, weight))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量标准化的后向公式实现\n",
    "# atch Norm 有两种模式：训练和eval模式。在训练模式下，样本统计数据是输入的函数。在eval模式中，我们使用保存的运行统计数据，它们不是输入的函数。这使得非训练模式的后向计算变得非常简单。下面我们仅实现和测试训练模式的情况。\n",
    "def unsqueeze_all(t):\n",
    "    # Helper function to ``unsqueeze`` all the dimensions that we reduce over\n",
    "    return t[None, :, None, None]\n",
    "\n",
    "def batch_norm_backward(grad_out, X, sum, sqrt_var, N, eps):\n",
    "    # We use the formula: ``out = (X - mean(X)) / (sqrt(var(X)) + eps)``\n",
    "    # in batch norm 2D forward. To simplify our derivation, we follow the\n",
    "    # chain rule and compute the gradients as follows before accumulating\n",
    "    # them all into a final grad_input.\n",
    "    #  1) ``grad of out wrt var(X)`` * ``grad of var(X) wrt X``\n",
    "    #  2) ``grad of out wrt mean(X)`` * ``grad of mean(X) wrt X``\n",
    "    #  3) ``grad of out wrt X in the numerator`` * ``grad of X wrt X``\n",
    "    # We then rewrite the formulas to use as few extra buffers as possible\n",
    "    tmp = ((X - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n",
    "    tmp *= -1\n",
    "    d_denom = tmp / (sqrt_var + eps)**2  # ``d_denom = -num / denom**2``\n",
    "    # It is useful to delete tensors when you no longer need them with ``del``\n",
    "    # For example, we could've done ``del tmp`` here because we won't use it later\n",
    "    # In this case, it's not a big difference because ``tmp`` only has size of (C,)\n",
    "    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n",
    "    d_var = d_denom / (2 * sqrt_var)  # ``denom = torch.sqrt(var) + eps``\n",
    "    # Compute ``d_mean_dx`` before allocating the final NCHW-sized grad_input buffer\n",
    "    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n",
    "    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n",
    "    # ``d_mean_dx`` has already been reassigned to a C-sized buffer so no need to worry\n",
    "\n",
    "    # ``(1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)``\n",
    "    grad_input = X * unsqueeze_all(d_var * N)\n",
    "    grad_input += unsqueeze_all(-d_var * sum)\n",
    "    grad_input *= 2 / ((N - 1) * N)\n",
    "    # (2) mean (see above)\n",
    "    grad_input += d_mean_dx\n",
    "    # (3) Add 'grad_out / <factor>' without allocating an extra buffer\n",
    "    grad_input *= unsqueeze_all(sqrt_var + eps)\n",
    "    grad_input += grad_out\n",
    "    grad_input /= unsqueeze_all(sqrt_var + eps)  # ``sqrt_var + eps > 0!``\n",
    "    return grad_input\n",
    "\n",
    "class BatchNorm(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, eps=1e-3):\n",
    "        # Don't save ``keepdim`` values for backward\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.save_for_backward(X)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_out):\n",
    "        X, = ctx.saved_tensors\n",
    "        return batch_norm_backward(grad_out, X, ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(1, 2, 3, 4, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(BatchNorm.apply, (a,), fast_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 融合卷积和 BatchNorm\n",
    "class FusedConvBN2DFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X, conv_weight, eps=1e-3):\n",
    "        assert X.ndim == 4  # N, C, H, W\n",
    "        # (1) Only need to save this single buffer for backward!\n",
    "        ctx.save_for_backward(X, conv_weight)\n",
    "\n",
    "        # (2) Exact same Conv2D forward from example above\n",
    "        X = F.conv2d(X, conv_weight)\n",
    "        # (3) Exact same BatchNorm2D forward from example above\n",
    "        sum = X.sum(dim=(0, 2, 3))\n",
    "        var = X.var(unbiased=True, dim=(0, 2, 3))\n",
    "        N = X.numel() / X.size(1)\n",
    "        sqrt_var = torch.sqrt(var)\n",
    "        ctx.eps = eps\n",
    "        ctx.sum = sum\n",
    "        ctx.N = N\n",
    "        ctx.sqrt_var = sqrt_var\n",
    "        mean = sum / N\n",
    "        denom = sqrt_var + eps\n",
    "        # Try to do as many things in-place as possible\n",
    "        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n",
    "        # avoids allocating one extra NCHW-sized buffer here\n",
    "        out = X - unsqueeze_all(mean)\n",
    "        out /= unsqueeze_all(denom)\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        X, conv_weight, = ctx.saved_tensors\n",
    "        # (4) Batch norm backward\n",
    "        # (5) We need to recompute conv\n",
    "        X_conv_out = F.conv2d(X, conv_weight)\n",
    "        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n",
    "                                       ctx.N, ctx.eps)\n",
    "        # (6) Conv2d backward\n",
    "        grad_X, grad_input = convolution_backward(grad_out, X, conv_weight)\n",
    "        return grad_X, grad_input, None, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 功能变体包装在有状态的 nn.Module中\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class FusedConvBN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n",
    "                 eps=1e-3, device=None, dtype=None):\n",
    "        super(FusedConvBN, self).__init__()\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        # Conv parameters\n",
    "        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.conv_weight = nn.Parameter(torch.empty(*weight_shape, **factory_kwargs))\n",
    "        # Batch norm parameters\n",
    "        num_features = out_channels\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        # Initialize\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return FusedConvBN2DFunction.apply(X, self.conv_weight, self.eps)\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        nn.init.kaiming_uniform_(self.conv_weight, a=math.sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weight = torch.rand(5, 3, 3, 3, requires_grad=True, dtype=torch.double)\n",
    "X = torch.rand(2, 3, 4, 4, requires_grad=True, dtype=torch.double)\n",
    "torch.autograd.gradcheck(FusedConvBN2DFunction.apply, (X, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Record memory allocated at the end of the forward pass\n",
    "memory_allocated = [[],[]]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, fused=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.fused = fused\n",
    "        if fused:\n",
    "            self.convbn1 = FusedConvBN(1, 32, 3)\n",
    "            self.convbn2 = FusedConvBN(32, 64, 3)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(32, affine=False, track_running_stats=False)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1, bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(64, affine=False, track_running_stats=False)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fused:\n",
    "            x = self.convbn1(x)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.bn1(x)\n",
    "        F.relu_(x)\n",
    "        if self.fused:\n",
    "            x = self.convbn2(x)\n",
    "        else:\n",
    "            x = self.conv2(x)\n",
    "            x = self.bn2(x)\n",
    "        F.relu_(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        F.relu_(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        F.relu_(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        if fused:\n",
    "            memory_allocated[0].append(torch.cuda.memory_allocated())\n",
    "        else:\n",
    "            memory_allocated[1].append(torch.cuda.memory_allocated())\n",
    "        return output\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 2 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Use inference mode instead of no_grad, for free improved test-time performance\n",
    "    with torch.inference_mode():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "train_kwargs = {'batch_size': 2048}\n",
    "test_kwargs = {'batch_size': 2048}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "dataset1 = datasets.MNIST('/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtcv/weiziyu/pytorch_tutorials/work_dirs', train=True, download=True,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtcv/weiziyu/pytorch_tutorials/work_dirs', train=False,download=True,\n",
    "                          transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.348850\n",
      "Train Epoch: 0 [4096/60000 (7%)]\tLoss: 7.905995\n",
      "Train Epoch: 0 [8192/60000 (13%)]\tLoss: 3.856884\n",
      "Train Epoch: 0 [12288/60000 (20%)]\tLoss: 2.177233\n",
      "Train Epoch: 0 [16384/60000 (27%)]\tLoss: 1.888144\n",
      "Train Epoch: 0 [20480/60000 (33%)]\tLoss: 1.754843\n",
      "Train Epoch: 0 [24576/60000 (40%)]\tLoss: 1.611404\n",
      "Train Epoch: 0 [28672/60000 (47%)]\tLoss: 1.590894\n",
      "Train Epoch: 0 [32768/60000 (53%)]\tLoss: 1.568939\n",
      "Train Epoch: 0 [36864/60000 (60%)]\tLoss: 1.310963\n",
      "Train Epoch: 0 [40960/60000 (67%)]\tLoss: 1.070456\n",
      "Train Epoch: 0 [45056/60000 (73%)]\tLoss: 0.996946\n",
      "Train Epoch: 0 [49152/60000 (80%)]\tLoss: 1.057302\n",
      "Train Epoch: 0 [53248/60000 (87%)]\tLoss: 0.887197\n",
      "Train Epoch: 0 [57344/60000 (93%)]\tLoss: 0.765099\n",
      "\n",
      "Test set: Average loss: 0.4127, Accuracy: 8950/10000 (90%)\n",
      "\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.349131\n",
      "Train Epoch: 0 [4096/60000 (7%)]\tLoss: 7.946282\n",
      "Train Epoch: 0 [8192/60000 (13%)]\tLoss: 3.230811\n",
      "Train Epoch: 0 [12288/60000 (20%)]\tLoss: 2.591554\n",
      "Train Epoch: 0 [16384/60000 (27%)]\tLoss: 1.946914\n",
      "Train Epoch: 0 [20480/60000 (33%)]\tLoss: 2.455155\n",
      "Train Epoch: 0 [24576/60000 (40%)]\tLoss: 1.995810\n",
      "Train Epoch: 0 [28672/60000 (47%)]\tLoss: 1.653510\n",
      "Train Epoch: 0 [32768/60000 (53%)]\tLoss: 1.275463\n",
      "Train Epoch: 0 [36864/60000 (60%)]\tLoss: 1.322699\n",
      "Train Epoch: 0 [40960/60000 (67%)]\tLoss: 1.479444\n",
      "Train Epoch: 0 [45056/60000 (73%)]\tLoss: 1.124814\n",
      "Train Epoch: 0 [49152/60000 (80%)]\tLoss: 0.884629\n",
      "Train Epoch: 0 [53248/60000 (87%)]\tLoss: 0.886491\n",
      "Train Epoch: 0 [57344/60000 (93%)]\tLoss: 0.897334\n",
      "\n",
      "Test set: Average loss: 0.5385, Accuracy: 8467/10000 (85%)\n",
      "\n",
      "cuDNN version: 90100\n",
      "\n",
      "Peak memory allocated:\n",
      "fused: 1.94GB, unfused: 1.50GB\n",
      "Memory allocated at end of forward pass:\n",
      "fused: 0.59GB, unfused: 0.96GB\n"
     ]
    }
   ],
   "source": [
    "# 内存使用情况比较\n",
    "from statistics import mean\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "if use_cuda:\n",
    "    peak_memory_allocated = []\n",
    "\n",
    "    for fused in (True, False):\n",
    "        torch.manual_seed(123456)\n",
    "\n",
    "        model = Net(fused=fused).to(device)\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "        scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "        for epoch in range(1):\n",
    "            train(model, device, train_loader, optimizer, epoch)\n",
    "            test(model, device, test_loader)\n",
    "            scheduler.step()\n",
    "        peak_memory_allocated.append(torch.cuda.max_memory_allocated())\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print()\n",
    "    print(\"Peak memory allocated:\")\n",
    "    print(f\"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB\")\n",
    "    print(\"Memory allocated at end of forward pass:\")\n",
    "    print(f\"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
