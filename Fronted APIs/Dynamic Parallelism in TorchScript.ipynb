{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 动态并行的两个重要 API 是：\n",
    "\n",
    "# torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -> torch.jit.Future[T]\n",
    "\n",
    "# torch.jit.wait(fut : torch.jit.Future[T]) -> T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.]), tensor([-1.]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def foo(x):\n",
    "    return torch.neg(x)\n",
    "\n",
    "\n",
    "# @torch.jit.script 是 PyTorch 中用于将 Python 函数或类转换为 TorchScript 的装饰器。TorchScript 是一种中间表示形式，允许 PyTorch 模型在不依赖 Python 解释器的情况下进行优化和执行。这使得模型可以在生产环境中更高效地运行，尤其是在部署到 C++ 环境或移动设备时。\n",
    "@torch.jit.script\n",
    "def example(x):\n",
    "    # Call `foo` using parallelism:\n",
    "    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n",
    "    future = torch.jit.fork(foo, x)\n",
    "\n",
    "    # Call `foo` normally\n",
    "    x_normal = foo(x)\n",
    "\n",
    "    # Second, we \"wait\" on the task. Since the task may be running in\n",
    "    # parallel, we have to \"wait\" for its result to become available.\n",
    "    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n",
    "    # call for a given Future, we can overlap computations so that they\n",
    "    # run in parallel.\n",
    "    x_parallel = torch.jit.wait(future)\n",
    "\n",
    "    return x_normal, x_parallel\n",
    "\n",
    "print(example(torch.ones(1))) # (-1., -1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fork()接受可调用函数fn和该可调用函数的参数args ，kwargs并创建一个异步任务来执行fn。 fn可以是函数、方法或 Module 实例。fork()返回对此执行结果值的引用，称为Future。由于fork在创建异步任务后立即返回，因此在执行调用fn后的代码行时可能尚未执行。因此，用于等待异步任务完成并返回值。fork()wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-100.)\n"
     ]
    }
   ],
   "source": [
    "# 这些结构可用于重叠函数内的语句的执行（如示例部分所示）或与其他语言结构（如循环）组合\n",
    "import torch\n",
    "from typing import List\n",
    "\n",
    "def foo(x):\n",
    "    return torch.neg(x)\n",
    "\n",
    "@torch.jit.script\n",
    "def example(x):\n",
    "    futures : List[torch.jit.Future[torch.Tensor]] = []\n",
    "    for _ in range(100):\n",
    "        futures.append(torch.jit.fork(foo, x))\n",
    "\n",
    "    results = []\n",
    "    for future in futures:\n",
    "        results.append(torch.jit.wait(future))\n",
    "\n",
    "    return torch.sum(torch.stack(results))\n",
    "\n",
    "print(example(torch.ones([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 0.6468088626861572  seconds\n"
     ]
    }
   ],
   "source": [
    "# 应用示例：双向 LSTM 集成\n",
    "import torch, time\n",
    "\n",
    "# In RNN parlance, the dimensions we care about are:\n",
    "# # of time-steps (T)\n",
    "# Batch size (B)\n",
    "# Hidden size/number of \"channels\" (C)\n",
    "T, B, C = 50, 50, 1024\n",
    "\n",
    "# A module that defines a single \"bidirectional LSTM\". This is simply two\n",
    "# LSTMs applied to the same sequence, but one in reverse\n",
    "class BidirectionalRecurrentLSTM(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
    "        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        # Forward layer\n",
    "        output_f, _ = self.cell_f(x)\n",
    "\n",
    "        # Backward layer. Flip input in the time dimension (dim 0), apply the\n",
    "        # layer, then flip the outputs in the time dimension\n",
    "        x_rev = torch.flip(x, dims=[0])\n",
    "        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n",
    "        output_b_rev = torch.flip(output_b, dims=[0])\n",
    "\n",
    "        return torch.cat((output_f, output_b_rev), dim=2)\n",
    "\n",
    "\n",
    "# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n",
    "# ensemble are run one-by-one on the same input then their results are\n",
    "# stacked and summed together, returning the combined result.\n",
    "class LSTMEnsemble(torch.nn.Module):\n",
    "    def __init__(self, n_models):\n",
    "        super().__init__()\n",
    "        self.n_models = n_models\n",
    "        self.models = torch.nn.ModuleList([\n",
    "            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        results = []\n",
    "        for model in self.models:\n",
    "            results.append(model(x))\n",
    "        return torch.stack(results).sum(dim=0)\n",
    "\n",
    "# For a head-to-head comparison to what we're going to do with fork/wait, let's\n",
    "# instantiate the model and compile it with TorchScript\n",
    "ens = torch.jit.script(LSTMEnsemble(n_models=4))\n",
    "\n",
    "# Normally you would pull this input out of an embedding table, but for the\n",
    "# purpose of this demo let's just use random data.\n",
    "x = torch.rand(T, B, C)\n",
    "\n",
    "# Let's run the model once to warm up things like the memory allocator\n",
    "ens(x)\n",
    "\n",
    "x = torch.rand(T, B, C)\n",
    "\n",
    "# Let's see how fast it runs!\n",
    "s = time.time()\n",
    "ens(x)\n",
    "print('Inference took', time.time() - s, ' seconds')\n",
    "# 并行化前向层和后向层\n",
    "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "    # Forward layer - fork() so this can run in parallel to the backward\n",
    "    # layer\n",
    "    future_f = torch.jit.fork(self.cell_f, x)\n",
    "\n",
    "    # Backward layer. Flip input in the time dimension (dim 0), apply the\n",
    "    # layer, then flip the outputs in the time dimension\n",
    "    x_rev = torch.flip(x, dims=[0])\n",
    "    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n",
    "    output_b_rev = torch.flip(output_b, dims=[0])\n",
    "\n",
    "    # Retrieve the output from the forward layer. Note this needs to happen\n",
    "    # *after* the stuff we want to parallelize with\n",
    "    output_f, _ = torch.jit.wait(future_f)\n",
    "\n",
    "    return torch.cat((output_f, output_b_rev), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 集成中的并行模型\n",
    "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "    # Launch tasks for each model\n",
    "    futures : List[torch.jit.Future[torch.Tensor]] = []\n",
    "    for model in self.models:\n",
    "        futures.append(torch.jit.fork(model, x))\n",
    "\n",
    "    # Collect the results from the launched tasks\n",
    "    results : List[torch.Tensor] = []\n",
    "    for future in futures:\n",
    "        results.append(torch.jit.wait(future))\n",
    "\n",
    "    return torch.stack(results).sum(dim=0)\n",
    "# 简洁版\n",
    "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "    futures = [torch.jit.fork(model, x) for model in self.models]\n",
    "    results = [torch.jit.wait(fut) for fut in futures]\n",
    "    return torch.stack(results).sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
