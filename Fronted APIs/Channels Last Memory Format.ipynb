{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.stride() 是一个方法，用于获取张量 x 的每个维度的步幅（stride）。\n",
    "# 步幅定义了在多维数组中，从一个元素移动到下一个元素的内存偏移量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1024, 32, 1)\n",
      "torch.Size([10, 3, 32, 32])\n",
      "(3072, 1, 96, 3)\n",
      "False\n",
      "(3072, 1024, 32, 1)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, C, H, W = 10, 3, 32, 32\n",
    "x = torch.empty(N, C, H, W)\n",
    "print(x.stride())  # Outputs: (3072, 1024, 32, 1)\n",
    "x = x.to(memory_format=torch.channels_last)\n",
    "print(x.shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved\n",
    "print(x.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "print(x.is_contiguous())\n",
    "x = x.to(memory_format=torch.contiguous_format)\n",
    "print(x.stride())  # Outputs: (3072, 1024, 32, 1)\n",
    "print(x.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1, 96, 3)\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Alternative option\n",
    "x = x.contiguous(memory_format=torch.channels_last)\n",
    "print(x.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "print(x.is_contiguous())\n",
    "print(x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 在 PyTorch 中，x.contiguous() 方法用于返回一个在内存中是连续的张量。理解这个概念对于优化内存访问和性能非常重要。以下是关于 x.contiguous() 的详细解释。\n",
    "\n",
    "# 1. 什么是连续的张量？\n",
    "# 在 PyTorch 中，张量的内存布局可以是连续的或非连续的。一个连续的张量表示其数据在内存中是连续存储的，这意味着可以一次性读取整个张量的数据。而非连续的张量则可能由于切片、转置或其他操作而导致数据在内存中不再是连续的。\n",
    "\n",
    "# 2. 为什么需要 contiguous()？\n",
    "# 某些操作和函数（例如，某些类型的视图操作、重塑操作等）要求输入张量是连续的。如果输入张量不是连续的，您可能会遇到错误或性能下降。在这种情况下，您可以使用 x.contiguous() 来确保张量是连续的。\n",
    "\n",
    "# 3. 使用 x.contiguous()\n",
    "# 当您调用 x.contiguous() 时，它会检查张量的内存布局。如果张量是连续的，它将返回原始张量；如果不是，它将创建一个新的连续张量，并将原始数据复制到这个新张量中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 建议用.to 而不是 .contiguous\n",
    "# 在 C=1 或 (H=1且W=1) 的情况下，tensor的连续性是模糊的\n",
    "# 这时候，调用 contiguous 成为一个无操作（no-op），并不会更新步幅（stride）。相反，to 方法会在维度大小为 1 的情况下，对张量进行重新步幅调整，以正确表示所需的内存格式\n",
    "special_x = torch.empty(4, 1, 4, 4)\n",
    "print(special_x.is_contiguous(memory_format=torch.channels_last))  # Outputs: True\n",
    "print(special_x.is_contiguous(memory_format=torch.contiguous_format))  # Outputs: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同样的事情也适用于显式置换 API permute。在可能出现歧义的特殊情况下，permute不能保证产生正确携带预期内存格式的步幅。我们建议使用to显式内存格式以避免意外行为。\n",
    "\n",
    "# 需要注意的是，在C==1 && H==1 && W==1极端情况下，当三个非批量维度都等于1（）时，当前实现无法将张量标记为通道最后的记忆格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1, 96, 3)\n",
      "(3072, 1, 96, 3)\n",
      "(3072, 1, 96, 3)\n",
      "(3072, 1, 96, 3)\n",
      "(3072, 1, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(N, C, H, W, memory_format=torch.channels_last)\n",
    "print(x.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "# clone保留内存格式\n",
    "y = x.clone()\n",
    "print(y.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "# to, cuda, float… 保留内存格式\n",
    "if torch.cuda.is_available():\n",
    "    y = x.cuda()\n",
    "    print(y.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "# empty_like,*_like运算符保留内存格式\n",
    "y = torch.empty_like(x)\n",
    "print(y.stride())  # Outputs: (3072, 1, 96, 3)\n",
    "# 逐点运算符保留内存格式\n",
    "z = x + y\n",
    "print(z.stride())  # Outputs: (3072, 1, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Conv，Batchnorm 使用支持通道last的cudnn backends最后（仅适用于cudnn >= 7.6）。卷积模块，不像二进制p-wise运算符，将通道last作为主要的内存格式。如果所有输入都是连续存储器格式，则运算符产生连续存储器格式的输出。否则，输出将采用通道的最后内存格式\n",
    "if torch.backends.cudnn.is_available() and torch.backends.cudnn.version() >= 7603:\n",
    "    model = torch.nn.Conv2d(8, 4, 3).cuda().half()\n",
    "    model = model.to(memory_format=torch.channels_last)  # Module parameters need to be channels last\n",
    "\n",
    "    input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float32, requires_grad=True)\n",
    "    input = input.to(device=\"cuda\", memory_format=torch.channels_last, dtype=torch.float16)\n",
    "\n",
    "    out = model(input)\n",
    "    print(out.is_contiguous(memory_format=torch.channels_last))  # Outputs: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
