{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常规自动微分API: Tensor.backward()、torch.autograd.grad\n",
    "# JAX启发的functional transform API\n",
    "    # Google JAX: https://github.com/jax-ml/jax\n",
    "    # torch.func: https://pytorch.org/docs/main/func.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(weight, bias, x):\n",
    "    return F.linear(x, weight, bias).tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 16\n",
    "weight = torch.randn(D, D)\n",
    "bias = torch.randn(D)\n",
    "x = torch.randn(D)  # feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n",
      "tensor([-0.5956, -0.6096, -0.1326, -0.2295,  0.4490,  0.3661, -0.1672, -1.1190,\n",
      "         0.1705, -0.6683,  0.1851,  0.1630,  0.0634,  0.6547,  0.5908, -0.1308])\n"
     ]
    }
   ],
   "source": [
    "# 我们必须每次使用不同的单位向量逐行计算它\n",
    "def compute_jac(xp):\n",
    "    jacobian_rows = [torch.autograd.grad(predict(weight, bias, xp), xp, vec)[0]\n",
    "                     for vec in unit_vectors]\n",
    "    return torch.stack(jacobian_rows)\n",
    "\n",
    "xp = x.clone().requires_grad_()\n",
    "unit_vectors = torch.eye(D)\n",
    "\n",
    "jacobian = compute_jac(xp)\n",
    "\n",
    "print(jacobian.shape)\n",
    "print(jacobian[0])  # show first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.vmap我们可以使用 PyTorch 的函数变换来摆脱 for 循环并矢量化计算\n",
    "from torch.func import vmap, vjp\n",
    "\n",
    "_, vjp_fn = vjp(partial(predict, weight, bias), x)\n",
    "\n",
    "ft_jacobian, = vmap(vjp_fn)(unit_vectors)\n",
    "\n",
    "# let's confirm both methods compute the same result\n",
    "assert torch.allclose(ft_jacobian, jacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.func.jacrev一个便捷函数来执行vmap-vjp组合以计算雅可比矩阵\n",
    "from torch.func import jacrev\n",
    "\n",
    "ft_jacobian = jacrev(predict, argnums=2)(weight, bias, x)\n",
    "\n",
    "# Confirm by running the following:\n",
    "assert torch.allclose(ft_jacobian, jacobian)\n",
    " \n",
    "# note the change in input via ``argnums`` parameters of 0,1 to map to weight and bias\n",
    "ft_jac_weight, ft_jac_bias = jacrev(predict, argnums=(0, 1))(weight, bias, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f4a73b3d8a0>\n",
      "compute_jac(xp)\n",
      "  1.35 ms\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f4a59bbe2f0>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  368.34 us\n",
      "  1 measurement, 500 runs , 1 thread\n",
      " Performance delta: 72.8105 percent improvement with vmap \n"
     ]
    }
   ],
   "source": [
    "# 比较性能\n",
    "def get_perf(first, first_descriptor, second, second_descriptor):\n",
    "    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n",
    "    faster = second.times[0]\n",
    "    slower = first.times[0]\n",
    "    gain = (slower-faster)/slower\n",
    "    if gain < 0: gain *=-1\n",
    "    final_gain = gain*100\n",
    "    print(f\" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} \")\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "without_vmap = Timer(stmt=\"compute_jac(xp)\", globals=globals())\n",
    "with_vmap = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "\n",
    "no_vmap_timer = without_vmap.timeit(500)\n",
    "with_vmap_timer = with_vmap.timeit(500)\n",
    "\n",
    "print(no_vmap_timer)\n",
    "print(with_vmap_timer)\n",
    "get_perf(no_vmap_timer, \"without vmap\",  with_vmap_timer, \"vmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入少于输出\n",
      "torch.Size([2048, 32])\n",
      "jacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a5793ffd0>\n",
      "jacfwd(predict, argnums=2)(weight, bias, x)\n",
      "  686.11 us\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "jacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a579591e0>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  18.99 ms\n",
      "  1 measurement, 500 runs , 1 thread\n",
      " Performance delta: 2667.2973 percent improvement with jacrev \n",
      "输入多于输出\n",
      "jacfwd time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a57925c00>\n",
      "jacfwd(predict, argnums=2)(weight, bias, x)\n",
      "  7.06 ms\n",
      "  1 measurement, 500 runs , 1 thread\n",
      "jacrev time: <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a59bbe380>\n",
      "jacrev(predict, argnums=2)(weight, bias, x)\n",
      "  467.05 us\n",
      "  1 measurement, 500 runs , 1 thread\n",
      " Performance delta: 1412.3726 percent improvement with jacfwd \n"
     ]
    }
   ],
   "source": [
    "# 反向jac和正向jac\n",
    "from torch.func import jacrev, jacfwd\n",
    "\n",
    "print(\"输入少于输出\")\n",
    "Din = 32\n",
    "Dout = 2048\n",
    "weight = torch.randn(Dout, Din)\n",
    "\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(Din)\n",
    "\n",
    "# remember the general rule about taller vs wider... here we have a taller matrix:\n",
    "print(weight.shape)\n",
    "\n",
    "using_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "using_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "\n",
    "jacfwd_timing = using_fwd.timeit(500)\n",
    "jacrev_timing = using_bwd.timeit(500)\n",
    "\n",
    "print(f'jacfwd time: {jacfwd_timing}')\n",
    "print(f'jacrev time: {jacrev_timing}')\n",
    "get_perf(jacfwd_timing, \"jacfwd\", jacrev_timing, \"jacrev\", )\n",
    "\n",
    "print(\"输入多于输出\")\n",
    "Din = 2048\n",
    "Dout = 32\n",
    "weight = torch.randn(Dout, Din)\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(Din)\n",
    "\n",
    "using_fwd = Timer(stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "using_bwd = Timer(stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n",
    "\n",
    "jacfwd_timing = using_fwd.timeit(500)\n",
    "jacrev_timing = using_bwd.timeit(500)\n",
    "\n",
    "print(f'jacfwd time: {jacfwd_timing}')\n",
    "print(f'jacrev time: {jacrev_timing}')\n",
    "get_perf(jacrev_timing, \"jacrev\", jacfwd_timing, \"jacfwd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hessians 是雅可比矩阵的雅可比矩阵（或偏导数的偏导数，又称二阶）。\n",
    "from torch.func import hessian\n",
    "\n",
    "# lets reduce the size in order not to overwhelm Colab. Hessians require\n",
    "# significant memory:\n",
    "Din = 512\n",
    "Dout = 32\n",
    "weight = torch.randn(Dout, Din)\n",
    "bias = torch.randn(Dout)\n",
    "x = torch.randn(Din)\n",
    "\n",
    "hess_api = hessian(predict, argnums=2)(weight, bias, x)\n",
    "hess_fwdfwd = jacfwd(jacfwd(predict, argnums=2), argnums=2)(weight, bias, x)\n",
    "hess_revrev = jacrev(jacrev(predict, argnums=2), argnums=2)(weight, bias, x)\n",
    "torch.allclose(hess_api, hess_fwdfwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight shape = torch.Size([33, 31])\n",
      "torch.Size([64, 33, 31])\n"
     ]
    }
   ],
   "source": [
    "# 批量雅可比矩阵和批量黑森矩阵\n",
    "# 如果您有多个输入参数，比如 func(x, y, z)，in_dims 的长度应该与输入参数的数量相同。\n",
    "# 对于每个输入参数，您可以指定：\n",
    "    # None：表示该输入参数不包含批量维度，函数将其视为单个样本。\n",
    "    # 一个整数：表示该输入参数的哪个维度是批量维度。例如，0 表示第一个维度是批量维度。\n",
    "    # 一个负数：表示从后向前数的维度。例如，-1 表示最后一个维度是批量维度\n",
    "batch_size = 64\n",
    "Din = 31\n",
    "Dout = 33\n",
    "\n",
    "weight = torch.randn(Dout, Din)\n",
    "print(f\"weight shape = {weight.shape}\")\n",
    "\n",
    "bias = torch.randn(Dout)\n",
    "\n",
    "x = torch.randn(batch_size, Din)\n",
    "\n",
    "compute_batch_jacobian = vmap(jacrev(predict, argnums=2), in_dims=(None, None, 0))\n",
    "batch_jacobian0 = compute_batch_jacobian(weight, bias, x)\n",
    "print(batch_jacobian0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 33, 31])\n"
     ]
    }
   ],
   "source": [
    "# 如果你有一个从 (B, N) -> (B, M) 的函数，并且确定每个输入都会产生一个独立的输出，那么有时也可以vmap通过对输出求和然后计算该函数的雅可比矩阵来做到这一点：\n",
    "# 因为别的bs的输出对于当前bs的输入的导数是0\n",
    "def predict_with_output_summed(weight, bias, x):\n",
    "    return predict(weight, bias, x).sum(0)\n",
    "\n",
    "batch_jacobian1 = jacrev(predict_with_output_summed, argnums=2)(weight, bias, x).movedim(1, 0)\n",
    "assert torch.allclose(batch_jacobian0, batch_jacobian1)\n",
    "print(batch_jacobian1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 33, 31, 31])"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_batch_hessian = vmap(hessian(predict, argnums=2), in_dims=(None, None, 0))\n",
    "\n",
    "batch_hess = compute_batch_hessian(weight, bias, x)\n",
    "batch_hess.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将反向模式 AD 与正向模式 AD 组合（而不是将反向模式与反向模式组合）通常是计算 hvp 的更节省内存的方法，因为正向模式 AD 不需要构建 Autograd 图并保存后向的中间体\n",
    "from torch.func import jvp, grad, vjp\n",
    "\n",
    "def hvp(f, primals, tangents):\n",
    "  return jvp(grad(f), primals, tangents)[1]\n",
    "def f(x):\n",
    "  return x.sin().sum()\n",
    "\n",
    "x = torch.randn(2048)\n",
    "tangent = torch.randn(2048)\n",
    "\n",
    "result = hvp(f, (x,), (tangent,))\n",
    "# 如果 PyTorch 前向 AD 无法覆盖你的操作，那么我们可以用反向模式 AD 来组合反向模式 AD：\n",
    "def hvp_revrev(f, primals, tangents):\n",
    "  _, vjp_fn = vjp(grad(f), *primals)\n",
    "  return vjp_fn(*tangents)\n",
    "\n",
    "result_hvp_revrev = hvp_revrev(f, (x,), (tangent,))\n",
    "assert torch.allclose(result, result_hvp_revrev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
