{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 在正向模式自动微分中，计算梯度是并行进行的，伴随于正向传递。也就是说，当我们计算输出时，方向导数也在同一时间计算。\n",
    "# 我们在计算时可以使用一个额外的张量来指定我们想要计算的方向，从而得出表达方向导数所需的梯度信息\n",
    "# 创建一个新的张量（即方向张量），它与原始输入一起使用，帮助计算最终的结果和梯度。\n",
    "# 输入张量被称为原始张量（primal），其主要是原始的数据输入。\n",
    "# 切向量表示与原始张量相关联的方向张量，表示了在计算过程中的变化\n",
    "# 最终的对偶张量捕获了对原始张量在特定方向上的微小变化或影响，并与输出的数值变化相关联"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd.forward_ad as fwAD\n",
    "\n",
    "primal = torch.randn(10, 10)\n",
    "tangent = torch.randn(10, 10)\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "# All forward AD computation must be performed in the context of\n",
    "# a ``dual_level`` context. All dual tensors created in such a context\n",
    "# will have their tangents destroyed upon exit. This is to ensure that\n",
    "# if the output or intermediate results of this computation are reused\n",
    "# in a future forward AD computation, their tangents (which are associated\n",
    "# with this computation) won't be confused with tangents from the later\n",
    "# computation.\n",
    "with fwAD.dual_level():\n",
    "    # To create a dual tensor we associate a tensor, which we call the\n",
    "    # primal with another tensor of the same size, which we call the tangent.\n",
    "    # If the layout of the tangent is different from that of the primal,\n",
    "    # The values of the tangent are copied into a new tensor with the same\n",
    "    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n",
    "    #\n",
    "    # It is also important to note that the dual tensor created by\n",
    "    # ``make_dual`` is a view of the primal.\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    assert fwAD.unpack_dual(dual_input).tangent is tangent\n",
    "\n",
    "    # To demonstrate the case where the copy of the tangent happens,\n",
    "    # we pass in a tangent with a layout different from that of the primal\n",
    "    dual_input_alt = fwAD.make_dual(primal, tangent.T)\n",
    "    assert fwAD.unpack_dual(dual_input_alt).tangent is not tangent\n",
    "\n",
    "    # Tensors that do not have an associated tangent are automatically\n",
    "    # considered to have a zero-filled tangent of the same shape.\n",
    "    plain_tensor = torch.randn(10, 10)\n",
    "    dual_output = fn(dual_input, plain_tensor)\n",
    "\n",
    "    # Unpacking the dual returns a ``namedtuple`` with ``primal`` and ``tangent``\n",
    "    # as attributes\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "\n",
    "assert fwAD.unpack_dual(dual_output).tangent is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 应用到nn.Module中，需要手动将parameters替换为dual_tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(5, 5)\n",
    "input = torch.randn(16, 5)\n",
    "\n",
    "params = {name: p for name, p in model.named_parameters()}\n",
    "tangents = {name: torch.rand_like(p) for name, p in params.items()}\n",
    "\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        delattr(model, name)\n",
    "        setattr(model, name, fwAD.make_dual(p, tangents[name]))\n",
    "\n",
    "    out = model(input)\n",
    "    jvp = fwAD.unpack_dual(out).tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call\n",
    "\n",
    "# We need a fresh module because the functional call requires the\n",
    "# the model to have parameters registered.\n",
    "model = nn.Linear(5, 5)\n",
    "\n",
    "dual_params = {}\n",
    "with fwAD.dual_level():\n",
    "    for name, p in params.items():\n",
    "        # Using the same ``tangents`` from the above section\n",
    "        dual_params[name] = fwAD.make_dual(p, tangents[name])\n",
    "    out = functional_call(model, dual_params, input)\n",
    "    jvp2 = fwAD.unpack_dual(out).tangent\n",
    "\n",
    "# Check our results\n",
    "assert torch.allclose(jvp, jvp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义导数\n",
    "class Fn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, foo):\n",
    "        result = torch.exp(foo)\n",
    "        # Tensors stored in ``ctx`` can be used in the subsequent forward grad\n",
    "        # computation.\n",
    "        ctx.result = result\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def jvp(ctx, gI):\n",
    "        gO = gI * ctx.result\n",
    "        # If the tensor stored in`` ctx`` will not also be used in the backward pass,\n",
    "        # one can manually free it using ``del``\n",
    "        del ctx.result\n",
    "        return gO\n",
    "\n",
    "fn = Fn.apply\n",
    "\n",
    "primal = torch.randn(10, 10, dtype=torch.double, requires_grad=True)\n",
    "tangent = torch.randn(10, 10)\n",
    "\n",
    "with fwAD.dual_level():\n",
    "    dual_input = fwAD.make_dual(primal, tangent)\n",
    "    dual_output = fn(dual_input)\n",
    "    jvp = fwAD.unpack_dual(dual_output).tangent\n",
    "\n",
    "# It is important to use ``autograd.gradcheck`` to verify that your\n",
    "# custom autograd Function computes the gradients correctly. By default,\n",
    "# ``gradcheck`` only checks the backward-mode (reverse-mode) AD gradients. Specify\n",
    "# ``check_forward_ad=True`` to also check forward grads. If you did not\n",
    "# implement the backward formula for your function, you can also tell ``gradcheck``\n",
    "# to skip the tests that require backward-mode AD by specifying\n",
    "# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n",
    "# ``check_batched_grad=False``.\n",
    "torch.autograd.gradcheck(Fn.apply, (primal,), check_forward_ad=True,\n",
    "                         check_backward_ad=False, check_undefined_grad=False,\n",
    "                         check_batched_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functorch: https://github.com/pytorch/functorch\n",
    "# forward-mode AD: https://pytorch.org/docs/main/notes/extending.html#forward-mode-ad\n",
    "# functorch transforms:  https://pytorch.org/functorch/stable/notebooks/jacobians_hessians.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/dolphinfs/hdd_pool/docker/user/hadoop-mtcv/weiziyu/miniconda3/envs/mllm/lib/python3.10/site-packages/torch/_functorch/deprecated.py:77: UserWarning:\n",
      "\n",
      "We've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用API\n",
    "import functorch as ft\n",
    "\n",
    "primal0 = torch.randn(10, 10)\n",
    "tangent0 = torch.randn(10, 10)\n",
    "primal1 = torch.randn(10, 10)\n",
    "tangent1 = torch.randn(10, 10)\n",
    "\n",
    "def fn(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "# Here is a basic example to compute the JVP of the above function.\n",
    "# The ``jvp(func, primals, tangents)`` returns ``func(*primals)`` as well as the\n",
    "# computed Jacobian-vector product (JVP). Each primal must be associated with a tangent of the same shape.\n",
    "primal_out, tangent_out = ft.jvp(fn, (primal0, primal1), (tangent0, tangent1))\n",
    "\n",
    "# ``functorch.jvp`` requires every primal to be associated with a tangent.\n",
    "# If we only want to associate certain inputs to `fn` with tangents,\n",
    "# then we'll need to create a new function that captures inputs without tangents:\n",
    "primal = torch.randn(10, 10)\n",
    "tangent = torch.randn(10, 10)\n",
    "y = torch.randn(10, 10)\n",
    "\n",
    "import functools\n",
    "new_fn = functools.partial(fn, y=y)\n",
    "primal_out, tangent_out = ft.jvp(new_fn, (primal,), (tangent,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(5, 5)\n",
    "input = torch.randn(16, 5)\n",
    "tangents = tuple([torch.rand_like(p) for p in model.parameters()])\n",
    "\n",
    "# Given a ``torch.nn.Module``, ``ft.make_functional_with_buffers`` extracts the state\n",
    "# (``params`` and buffers) and returns a functional version of the model that\n",
    "# can be invoked like a function.\n",
    "# That is, the returned ``func`` can be invoked like\n",
    "# ``func(params, buffers, input)``.\n",
    "# ``ft.make_functional_with_buffers`` is analogous to the ``nn.Modules`` stateless API\n",
    "# that you saw previously and we're working on consolidating the two.\n",
    "func, params, buffers = ft.make_functional_with_buffers(model)\n",
    "\n",
    "# Because ``jvp`` requires every input to be associated with a tangent, we need to\n",
    "# create a new function that, when given the parameters, produces the output\n",
    "def func_params_only(params):\n",
    "    return func(params, buffers, input)\n",
    "\n",
    "model_output, jvp_out = ft.jvp(func_params_only, (params,), (tangents,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用nn模块与functorch。为了计算关于模型参数的雅可比向量积，我们需要重新表述nn。模块作为一个函数，接受模型参数和模块的输入。\n",
    "model = nn.Linear(5, 5)\n",
    "input = torch.randn(16, 5)\n",
    "tangents = tuple([torch.rand_like(p) for p in model.parameters()])\n",
    "\n",
    "# Given a ``torch.nn.Module``, ``ft.make_functional_with_buffers`` extracts the state\n",
    "# (``params`` and buffers) and returns a functional version of the model that\n",
    "# can be invoked like a function.\n",
    "# That is, the returned ``func`` can be invoked like\n",
    "# ``func(params, buffers, input)``.\n",
    "# ``ft.make_functional_with_buffers`` is analogous to the ``nn.Modules`` stateless API\n",
    "# that you saw previously and we're working on consolidating the two.\n",
    "func, params, buffers = ft.make_functional_with_buffers(model)\n",
    "\n",
    "# Because ``jvp`` requires every input to be associated with a tangent, we need to\n",
    "# create a new function that, when given the parameters, produces the output\n",
    "def func_params_only(params):\n",
    "    return func(params, buffers, input)\n",
    "\n",
    "model_output, jvp_out = ft.jvp(func_params_only, (params,), (tangents,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
