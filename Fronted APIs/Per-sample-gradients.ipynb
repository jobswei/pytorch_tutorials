{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 样本梯度计算是指计算一批数据中每个样本的梯度。它是差分隐私、元学习和优化研究中的有用量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Here's a simple CNN and loss function:\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "def loss_fn(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "num_models = 10\n",
    "batch_size = 64\n",
    "data = torch.randn(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "targets = torch.randint(10, (64,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常规\n",
    "model = SimpleCNN().to(device=device)\n",
    "predictions = model(data)  # move the entire mini-batch through the model\n",
    "\n",
    "loss = loss_fn(predictions, targets)\n",
    "loss.backward()  # back propagate the 'average' gradient of this mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个样本\n",
    "def compute_grad(sample, target):\n",
    "    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n",
    "    target = target.unsqueeze(0)\n",
    "\n",
    "    prediction = model(sample)\n",
    "    loss = loss_fn(prediction, target)\n",
    "\n",
    "    return torch.autograd.grad(loss, list(model.parameters()))\n",
    "\n",
    "\n",
    "def compute_sample_grads(data, targets):\n",
    "    \"\"\" manually process each sample with per sample gradient \"\"\"\n",
    "    sample_grads = [compute_grad(data[i], targets[i]) for i in range(batch_size)]\n",
    "    sample_grads = zip(*sample_grads)\n",
    "    sample_grads = [torch.stack(shards) for shards in sample_grads]\n",
    "    return sample_grads\n",
    "\n",
    "per_sample_grads = compute_sample_grads(data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0037, device='cuda:0')\n",
      "tensor(0.0013, device='cuda:0')\n",
      "tensor(0.0120, device='cuda:0')\n",
      "tensor(0.0060, device='cuda:0')\n",
      "tensor(0.0107, device='cuda:0')\n",
      "tensor(0.0112, device='cuda:0')\n",
      "tensor(0.0001, device='cuda:0')\n",
      "tensor(4.4703e-06, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 高效的方法，使用函数变换\n",
    "from torch.func import functional_call, vmap, grad\n",
    "\n",
    "params = {k: v.detach() for k, v in model.named_parameters()}\n",
    "buffers = {k: v.detach() for k, v in model.named_buffers()}\n",
    "def compute_loss(params, buffers, sample, target):\n",
    "    batch = sample.unsqueeze(0)\n",
    "    targets = target.unsqueeze(0)\n",
    "\n",
    "    predictions = functional_call(model, (params, buffers), (batch,))\n",
    "    loss = loss_fn(predictions, targets)\n",
    "    return loss\n",
    "ft_compute_grad = grad(compute_loss)\n",
    "ft_compute_sample_grad = vmap(ft_compute_grad, in_dims=(None, None, 0, 0))\n",
    "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, data, targets)\n",
    "for per_sample_grad, ft_per_sample_grad in zip(per_sample_grads, ft_per_sample_grads.values()):\n",
    "    print(torch.max(per_sample_grad-ft_per_sample_grad))\n",
    "    # assert torch.allclose(per_sample_grad, ft_per_sample_grad, atol=3e-3, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0037, device='cuda:0')"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(per_sample_grad-ft_per_sample_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-sample-grads without vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a579f8130>\n",
      "compute_sample_grads(data, targets)\n",
      "  75.80 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "Per-sample-grads with vmap <torch.utils.benchmark.utils.common.Measurement object at 0x7f4a579aafb0>\n",
      "ft_compute_sample_grad(params, buffers, data, targets)\n",
      "  3.04 ms\n",
      "  1 measurement, 100 runs , 1 thread\n",
      "Performance delta: 2393.6559 percent improvement with vmap \n"
     ]
    }
   ],
   "source": [
    "def get_perf(first, first_descriptor, second, second_descriptor):\n",
    "    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n",
    "    second_res = second.times[0]\n",
    "    first_res = first.times[0]\n",
    "\n",
    "    gain = (first_res-second_res)/first_res\n",
    "    if gain < 0: gain *=-1\n",
    "    final_gain = gain*100\n",
    "\n",
    "    print(f\"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n",
    "\n",
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "without_vmap = Timer(stmt=\"compute_sample_grads(data, targets)\", globals=globals())\n",
    "with_vmap = Timer(stmt=\"ft_compute_sample_grad(params, buffers, data, targets)\",globals=globals())\n",
    "no_vmap_timing = without_vmap.timeit(100)\n",
    "with_vmap_timing = with_vmap.timeit(100)\n",
    "\n",
    "print(f'Per-sample-grads without vmap {no_vmap_timing}')\n",
    "print(f'Per-sample-grads with vmap {with_vmap_timing}')\n",
    "\n",
    "get_perf(with_vmap_timing, \"vmap\", no_vmap_timing, \"no vmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
